# GestureJarvis

**Reconocimiento de gestos con webcam + acciones del sistema** ‚Äî Proyecto con OpenCV, MediaPipe y PyTorch.

GestureJarvis captura v√≠deo en tiempo real, detecta la mano, clasifica gestos mediante un modelo MLP entrenado con PyTorch, y ejecuta acciones del sistema operativo (volumen, rat√≥n, click).

---

## Gestos soportados

| Gesto | Acci√≥n |
|-------|--------|
| üëç `THUMBS_UP` | Subir volumen del sistema |
| üëé `THUMBS_DOWN` | Bajar volumen del sistema |
| ‚òùÔ∏è `INDEX_POINT` | Activar modo rat√≥n (mover cursor con el dedo √≠ndice) |
| ü§è `PINCH` | Click izquierdo (pellizco pulgar + √≠ndice) |

---

## Pipeline completo

```
Webcam ‚Üí MediaPipe Hands ‚Üí 21 landmarks (63 features)
    ‚Üí Normalizaci√≥n (origen en mu√±eca + escala)
        ‚Üí StandardScaler ‚Üí MLP (PyTorch) ‚Üí Predicci√≥n + Confianza
            ‚Üí Si confianza > 80%: ejecutar acci√≥n del sistema
```

---

## Estructura del proyecto

```
gesture-jarvis/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ raw_samples.csv          # Dataset recolectado
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ gesture_model.pt         # Modelo PyTorch entrenado
‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl               # StandardScaler (sklearn)
‚îÇ   ‚îî‚îÄ‚îÄ label_encoder.pkl        # LabelEncoder (sklearn)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                 # MediaPipe, landmarks, normalizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ collect_data.py          # Recolecci√≥n de muestras con webcam
‚îÇ   ‚îú‚îÄ‚îÄ train_model.py           # Entrenamiento del MLP
‚îÇ   ‚îú‚îÄ‚îÄ realtime_demo.py         # Demo en tiempo real
‚îÇ   ‚îî‚îÄ‚îÄ actions.py               # Acciones del sistema (volumen, rat√≥n)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## Requisitos previos

- **Windows 10/11**
- **Python 3.11**
- **Webcam** funcional
- **Conda** instalado (Anaconda o Miniconda)

---

## Instalaci√≥n

### 1. Crear entorno conda

```bash
conda create -n gesturejarvis python=3.11 -y
conda activate gesturejarvis
```

### 2. Instalar dependencias

```bash
cd gesture-jarvis
pip install -r requirements.txt
```

> **Nota GPU:** El proyecto funciona **sin GPU**. Si tienes una GPU NVIDIA compatible, instala la versi√≥n CUDA de PyTorch:
> ```bash
> pip install torch --index-url https://download.pytorch.org/whl/cu121
> ```

### 3. Verificar instalaci√≥n

```bash
python -c "import cv2, mediapipe, torch, pyautogui; print('Todo OK'); print(f'CUDA: {torch.cuda.is_available()}')"
```

---

## Uso

El flujo de trabajo tiene 3 pasos: **recolectar datos ‚Üí entrenar ‚Üí ejecutar demo**.

### Paso 1: Recolectar dataset

```bash
python src/collect_data.py
```

**Controles:**
| Tecla | Acci√≥n |
|-------|--------|
| `1` | Seleccionar etiqueta `THUMBS_UP` |
| `2` | Seleccionar etiqueta `THUMBS_DOWN` |
| `3` | Seleccionar etiqueta `INDEX_POINT` |
| `4` | Seleccionar etiqueta `PINCH` |
| `ESPACIO` | Guardar muestra actual |
| `Q` | Salir |

**Recomendaciones para el dataset:**

- **M√≠nimo 200 muestras por gesto** (800 total).
- Lo ideal son **300-500 por gesto**.
- Var√≠a la posici√≥n de la mano (centro, izquierda, derecha, arriba, abajo).
- Var√≠a la distancia a la c√°mara (cerca, medio, lejos).
- Var√≠a ligeramente la orientaci√≥n de la mano.
- Graba con la iluminaci√≥n habitual de tu entorno.
- Usa ambas manos si quieres soporte para las dos.
- Las muestras se acumulan en `data/raw_samples.csv`; puedes ejecutar el script m√∫ltiples veces.

### Paso 2: Entrenar el modelo

```bash
python src/train_model.py
```

Salida esperada:
- **Accuracy** en el conjunto de test.
- **Classification report** con precision/recall por gesto.
- Artefactos guardados en `models/`.

Si el accuracy es bajo (<90%), recolecta m√°s muestras o revisa que los gestos sean suficientemente diferentes entre s√≠.

### Paso 3: Ejecutar la demo en tiempo real

```bash
python src/realtime_demo.py
```

La ventana mostrar√°:
- Los landmarks de la mano dibujados.
- El gesto predicho y su confianza.
- Si el modo rat√≥n est√° `ON` o `OFF`.

**Pulsa `Q` para salir.**

---

## Detalles t√©cnicos

### Normalizaci√≥n de landmarks

Para que el modelo sea **robusto a posici√≥n y escala**:

1. Se resta la posici√≥n de la **mu√±eca** (landmark 0) como origen ‚Üí elimina dependencia de la posici√≥n en el frame.
2. Se divide por la **distancia mu√±eca ‚Üí MIDDLE_MCP** (landmark 9) ‚Üí elimina dependencia del tama√±o de la mano / distancia a la c√°mara.

### Modelo MLP

```
Input (63) ‚Üí Linear(128) ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout(0.3)
           ‚Üí Linear(64)  ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout(0.3)
           ‚Üí Linear(num_classes)
```

Se entrena con **Adam** (lr=0.001), **CrossEntropyLoss**, durante **50 epochs**.

### Modo rat√≥n

- Se activa solo si `INDEX_POINT` se mantiene **‚â• 0.5 segundos** continuos.
- La posici√≥n del landmark 8 (punta del √≠ndice) se mapea a coordenadas de pantalla.
- Se aplica un **moving average** (ventana de 5 frames) para suavizar el movimiento.

### Click con PINCH

- Solo se ejecuta en la **transici√≥n** de "no pinch" ‚Üí "pinch".
- Mientras se mantiene el gesto, **no se repite** el click.
- Tiene un **cooldown de 0.5 segundos**.

### Control de volumen

- Usa **pycaw** (COM API de Windows) para controlar el volumen del sistema.
- Cada acci√≥n cambia un **5%** del volumen.
- Cooldown de **0.3 segundos** para evitar cambios bruscos.

---

## Soluci√≥n de problemas

| Problema | Soluci√≥n |
|----------|----------|
| `No se pudo abrir la webcam` | Verifica que la webcam est√° conectada y no la usa otra app |
| `Modelo no encontrado` | Ejecuta `python src/train_model.py` primero |
| `Accuracy muy baja` | Recolecta m√°s muestras (‚â•200/gesto) con variedad |
| `El rat√≥n se mueve err√°ticamente` | Aumenta `MOUSE_SMOOTH_WINDOW` en `realtime_demo.py` |
| `El volumen no cambia` | Ejecuta como administrador si los permisos COM fallan |
| `Error de importaci√≥n pycaw` | Aseg√∫rate de tener `comtypes` instalado: `pip install comtypes` |

---

## Licencia

Proyecto educativo ‚Äî Curso EOI. Pedro Manuel C√≥zar Ortiz.
