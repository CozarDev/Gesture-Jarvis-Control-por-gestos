{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a485b761",
   "metadata": {},
   "source": [
    "# ğŸ–ï¸ GestureJarvis â€” Tutorial Completo\n",
    "\n",
    "## Reconocimiento de Gestos con MediaPipe + PyTorch\n",
    "\n",
    "Este notebook te guiarÃ¡ paso a paso por:\n",
    "1. **RecolecciÃ³n de datos** con webcam y MediaPipe\n",
    "2. **Preprocesamiento** y normalizaciÃ³n de landmarks\n",
    "3. **Entrenamiento** de un modelo MLP en PyTorch\n",
    "4. **EvaluaciÃ³n** y anÃ¡lisis del modelo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c55584",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1. Importar LibrerÃ­as\n",
    "\n",
    "Primero importamos todas las dependencias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684eece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Configurar matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas correctamente\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956af352",
   "metadata": {},
   "source": [
    "## ğŸ¯ 2. ConfiguraciÃ³n del Proyecto\n",
    "\n",
    "Definimos las rutas y constantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas del proyecto\n",
    "PROJECT_ROOT = Path().absolute()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Archivos\n",
    "CSV_PATH = DATA_DIR / \"raw_samples.csv\"\n",
    "MODEL_PATH = MODELS_DIR / \"gesture_model.pt\"\n",
    "SCALER_PATH = MODELS_DIR / \"scaler.pkl\"\n",
    "ENCODER_PATH = MODELS_DIR / \"label_encoder.pkl\"\n",
    "\n",
    "# AÃ±adir src al path para imports\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Constantes\n",
    "NUM_LANDMARKS = 21\n",
    "FEATURES_PER_LANDMARK = 3  # x, y, z\n",
    "TOTAL_FEATURES = NUM_LANDMARKS * FEATURES_PER_LANDMARK  # 63\n",
    "\n",
    "# Gestos que vamos a reconocer\n",
    "GESTURES = {\n",
    "    1: \"THUMBS_UP\",\n",
    "    2: \"THUMBS_DOWN\",\n",
    "    3: \"INDEX_POINT\",\n",
    "    4: \"PINCH\"\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“ Directorio del proyecto: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ“Š Dataset: {CSV_PATH}\")\n",
    "print(f\"ğŸ§  Modelo: {MODEL_PATH}\")\n",
    "print(f\"\\nğŸ¯ Gestos a reconocer: {list(GESTURES.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c631",
   "metadata": {},
   "source": [
    "## ğŸ¤š 3. MediaPipe Hands â€” DetecciÃ³n de Landmarks\n",
    "\n",
    "MediaPipe detecta **21 landmarks** (puntos clave) en cada mano:\n",
    "\n",
    "```\n",
    "     8   12  16  20\n",
    "     |   |   |   |\n",
    "  4--7-11-15-19\n",
    "  |  |   |   |   \n",
    "  3--6-10-14-18\n",
    "  |  |   |   |   \n",
    "  2--5--9-13-17\n",
    "  |  \n",
    "  1  \n",
    "  |  \n",
    "  0 (muÃ±eca)\n",
    "```\n",
    "\n",
    "Cada landmark tiene 3 coordenadas: **(x, y, z)**\n",
    "- `x, y`: posiciÃ³n normalizada (0-1) en el frame\n",
    "- `z`: profundidad relativa\n",
    "\n",
    "**Total: 21 Ã— 3 = 63 features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919cdbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import init_mediapipe, detect_hand, extract_landmarks, normalize_landmarks\n",
    "\n",
    "# Inicializar MediaPipe\n",
    "landmarker = init_mediapipe(max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "print(\"âœ… MediaPipe HandLandmarker inicializado\")\n",
    "print(f\"   Detecta hasta 1 mano por frame\")\n",
    "print(f\"   Confianza mÃ­nima: 70%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc17131",
   "metadata": {},
   "source": [
    "## ğŸ“¸ 4. RecolecciÃ³n de Datos\n",
    "\n",
    "### Proceso:\n",
    "1. Capturar frame de la webcam\n",
    "2. Detectar mano con MediaPipe\n",
    "3. Extraer 63 features (landmarks)\n",
    "4. **Normalizar** para robustez\n",
    "5. Guardar en CSV con etiqueta\n",
    "\n",
    "### âš ï¸ Importante: NormalizaciÃ³n\n",
    "\n",
    "**Â¿Por quÃ© normalizar?**\n",
    "- La posiciÃ³n de la mano en el frame varÃ­a (izquierda, derecha, arriba, abajo)\n",
    "- La distancia a la cÃ¡mara cambia el tamaÃ±o\n",
    "- El modelo debe ser **invariante a posiciÃ³n y escala**\n",
    "\n",
    "**MÃ©todo:**\n",
    "1. **TraslaciÃ³n**: Restar la muÃ±eca (landmark 0) â†’ origen en la muÃ±eca\n",
    "2. **Escalado**: Dividir por dist(muÃ±eca â†’ middle_mcp) â†’ tamaÃ±o normalizado\n",
    "\n",
    "```python\n",
    "# Antes: landmarks dependen de posiciÃ³n/tamaÃ±o\n",
    "# DespuÃ©s: solo importa la FORMA del gesto\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c044dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_samples_interactive():\n",
    "    \"\"\"\n",
    "    FunciÃ³n interactiva para recolectar muestras con la webcam.\n",
    "    \n",
    "    Controles:\n",
    "        1-4: Cambiar gesto\n",
    "        SPACE: Guardar muestra\n",
    "        Q: Salir\n",
    "    \"\"\"\n",
    "    from src.utils import draw_landmarks, put_text, get_csv_columns\n",
    "    \n",
    "    # Preparar CSV\n",
    "    csv_exists = CSV_PATH.exists() and CSV_PATH.stat().st_size > 0\n",
    "    csv_file = open(CSV_PATH, mode=\"a\", newline=\"\", encoding=\"utf-8\")\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "    if not csv_exists:\n",
    "        writer.writerow(get_csv_columns())\n",
    "        csv_file.flush()\n",
    "    \n",
    "    # Contar muestras existentes\n",
    "    sample_count = 0\n",
    "    if csv_exists:\n",
    "        with open(CSV_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            sample_count = max(0, sum(1 for _ in f) - 1)\n",
    "    \n",
    "    current_label = \"THUMBS_UP\"\n",
    "    \n",
    "    # Abrir webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"âŒ Error: No se pudo abrir la webcam\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"  ğŸ¥ RECOLECCIÃ“N DE DATOS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"  Controles:\")\n",
    "    print(\"    1 = THUMBS_UP\")\n",
    "    print(\"    2 = THUMBS_DOWN\")\n",
    "    print(\"    3 = INDEX_POINT\")\n",
    "    print(\"    4 = PINCH\")\n",
    "    print(\"    SPACE = Guardar muestra\")\n",
    "    print(\"    Q = Salir\")\n",
    "    print(f\"  Muestras actuales: {sample_count}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    frame_idx = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, _ = frame.shape\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detectar mano\n",
    "        frame_idx += 1\n",
    "        timestamp_ms = frame_idx * 33\n",
    "        result = detect_hand(landmarker, rgb, timestamp_ms)\n",
    "        \n",
    "        landmarks_ready = False\n",
    "        norm_landmarks = None\n",
    "        \n",
    "        if result is not None and result.hand_landmarks:\n",
    "            hand_lms = result.hand_landmarks[0]\n",
    "            draw_landmarks(frame, hand_lms)\n",
    "            raw = extract_landmarks(hand_lms)\n",
    "            norm_landmarks = normalize_landmarks(raw)\n",
    "            landmarks_ready = True\n",
    "        \n",
    "        # Overlay\n",
    "        put_text(frame, f\"Gesto: {current_label}\", (10, 30), 0.8, (0, 255, 255))\n",
    "        put_text(frame, f\"Muestras: {sample_count}\", (10, 65), 0.7, (255, 255, 255))\n",
    "        status = \"âœ“ Mano detectada\" if landmarks_ready else \"âœ— Sin mano\"\n",
    "        color = (0, 255, 0) if landmarks_ready else (0, 0, 255)\n",
    "        put_text(frame, status, (10, 100), 0.7, color)\n",
    "        put_text(frame, \"SPACE=guardar | 1-4=gesto | Q=salir\", (10, h - 20), 0.5, (200, 200, 200))\n",
    "        \n",
    "        cv2.imshow(\"GestureJarvis - RecolecciÃ³n\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # Cambiar gesto\n",
    "        if key in [ord('1'), ord('2'), ord('3'), ord('4')]:\n",
    "            idx = int(chr(key))\n",
    "            current_label = GESTURES[idx]\n",
    "            print(f\"  ğŸ¯ Gesto â†’ {current_label}\")\n",
    "        \n",
    "        # Guardar\n",
    "        elif key == ord(' '):\n",
    "            if landmarks_ready and norm_landmarks is not None:\n",
    "                row = norm_landmarks.tolist() + [current_label]\n",
    "                writer.writerow(row)\n",
    "                csv_file.flush()\n",
    "                sample_count += 1\n",
    "                print(f\"  âœ… Muestra #{sample_count} guardada â€” {current_label}\")\n",
    "            else:\n",
    "                print(\"  âš ï¸  Sin mano detectada\")\n",
    "        \n",
    "        # Salir\n",
    "        elif key in [ord('q'), ord('Q')]:\n",
    "            break\n",
    "    \n",
    "    csv_file.close()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    landmarker.close()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total de muestras: {sample_count}\")\n",
    "    print(f\"ğŸ’¾ Guardadas en: {CSV_PATH}\")\n",
    "\n",
    "# âš ï¸ Descomenta la siguiente lÃ­nea para ejecutar la recolecciÃ³n\n",
    "# collect_samples_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674c096",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Recomendaciones para el Dataset\n",
    "\n",
    "Para un modelo robusto, recolecta:\n",
    "- **MÃ­nimo**: 200 muestras por gesto (800 total)\n",
    "- **Ideal**: 300-500 por gesto (1200-2000 total)\n",
    "\n",
    "**Variabilidad importante:**\n",
    "- âœ… Diferentes posiciones (centro, izquierda, derecha, arriba, abajo)\n",
    "- âœ… Diferentes distancias a la cÃ¡mara\n",
    "- âœ… Ligeras variaciones en la orientaciÃ³n de la mano\n",
    "- âœ… Diferentes condiciones de iluminaciÃ³n\n",
    "- âœ… Ambas manos si quieres soportar las dos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe714cb",
   "metadata": {},
   "source": [
    "## ğŸ“Š 5. AnÃ¡lisis Exploratorio del Dataset\n",
    "\n",
    "Cargamos y exploramos los datos recolectados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ab655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "if not CSV_PATH.exists():\n",
    "    print(\"âš ï¸  Dataset no encontrado. Ejecuta primero collect_samples_interactive()\")\n",
    "else:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset cargado: {len(df)} muestras\")\n",
    "    print(f\"ğŸ“ Dimensiones: {df.shape}\")\n",
    "    print(f\"\\nğŸ·ï¸  Columnas: {df.shape[1]} ({df.shape[1]-1} features + 1 label)\")\n",
    "    \n",
    "    # DistribuciÃ³n de clases\n",
    "    print(\"\\nğŸ“ˆ DistribuciÃ³n de gestos:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    # Visualizar distribuciÃ³n\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # GrÃ¡fico de barras\n",
    "    df['label'].value_counts().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "    axes[0].set_title('DistribuciÃ³n de Gestos', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Gesto')\n",
    "    axes[0].set_ylabel('Cantidad de Muestras')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # GrÃ¡fico de pastel\n",
    "    df['label'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90)\n",
    "    axes[1].set_title('ProporciÃ³n de Gestos', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Verificar balance\n",
    "    counts = df['label'].value_counts()\n",
    "    if counts.max() / counts.min() > 2:\n",
    "        print(\"\\nâš ï¸  Dataset desbalanceado. Considera recolectar mÃ¡s muestras del gesto minoritario.\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Dataset bien balanceado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EstadÃ­sticas de las features\n",
    "if CSV_PATH.exists():\n",
    "    print(\"ğŸ“Š EstadÃ­sticas de las Features (primeras 10):\")\n",
    "    print(df.iloc[:, :10].describe())\n",
    "    \n",
    "    # Visualizar algunas features\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('DistribuciÃ³n de Landmarks (x0, y0, x1, y1)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    features = ['x0', 'y0', 'x1', 'y1']\n",
    "    for idx, (ax, feat) in enumerate(zip(axes.flatten(), features)):\n",
    "        for gesture in df['label'].unique():\n",
    "            data = df[df['label'] == gesture][feat]\n",
    "            ax.hist(data, bins=30, alpha=0.6, label=gesture)\n",
    "        ax.set_title(f'Landmark {feat}', fontweight='bold')\n",
    "        ax.set_xlabel('Valor')\n",
    "        ax.set_ylabel('Frecuencia')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476119c",
   "metadata": {},
   "source": [
    "## ğŸ§  6. PreparaciÃ³n de Datos para Entrenamiento\n",
    "\n",
    "### Pipeline de preprocesamiento:\n",
    "1. Separar features (X) y labels (y)\n",
    "2. Codificar labels â†’ nÃºmeros (0, 1, 2, 3)\n",
    "3. Split train/test (80/20)\n",
    "4. Estandarizar features con **StandardScaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CSV_PATH.exists():\n",
    "    # Separar X e y\n",
    "    X = df.iloc[:, :-1].values.astype(np.float32)  # 63 features\n",
    "    y = df['label'].values\n",
    "    \n",
    "    print(f\"ğŸ“Š X shape: {X.shape}\")\n",
    "    print(f\"ğŸ·ï¸  y shape: {y.shape}\")\n",
    "    print(f\"\\nğŸ¯ Clases Ãºnicas: {np.unique(y)}\")\n",
    "    \n",
    "    # Codificar labels\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    num_classes = len(le.classes_)\n",
    "    \n",
    "    print(f\"\\nğŸ”¢ Label Encoding:\")\n",
    "    for i, label in enumerate(le.classes_):\n",
    "        print(f\"   {label} â†’ {i}\")\n",
    "    \n",
    "    # Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_enc, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y_enc\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“‚ Split realizado:\")\n",
    "    print(f\"   Train: {len(X_train)} muestras ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {len(X_test)} muestras ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Estandarizar\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nğŸ“ EstandarizaciÃ³n:\")\n",
    "    print(f\"   Media train: {X_train_scaled.mean():.6f}\")\n",
    "    print(f\"   Std train: {X_train_scaled.std():.6f}\")\n",
    "    print(f\"   Media test: {X_test_scaled.mean():.6f}\")\n",
    "    print(f\"   Std test: {X_test_scaled.std():.6f}\")\n",
    "    \n",
    "    print(\"\\nâœ… Datos preparados para entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410788af",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ 7. Arquitectura del Modelo\n",
    "\n",
    "### Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Red neuronal completamente conectada con 3 capas:\n",
    "\n",
    "```\n",
    "Input (63)  â†’  [Linear(128) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.3)]\n",
    "            â†’  [Linear(64)  â†’ BatchNorm â†’ ReLU â†’ Dropout(0.3)]\n",
    "            â†’  [Linear(num_classes)]\n",
    "            â†’  Softmax (implÃ­cito en CrossEntropyLoss)\n",
    "```\n",
    "\n",
    "**Componentes:**\n",
    "- **BatchNorm**: Estabiliza el entrenamiento\n",
    "- **ReLU**: FunciÃ³n de activaciÃ³n no lineal\n",
    "- **Dropout**: RegularizaciÃ³n (evita overfitting)\n",
    "- **CrossEntropyLoss**: funciÃ³n de pÃ©rdida para clasificaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936841c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureMLP(nn.Module):\n",
    "    \"\"\"Red neuronal MLP de 3 capas para clasificaciÃ³n de gestos.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=63, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Capa 1: 63 â†’ 128\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Capa 2: 128 â†’ 64\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Capa 3: 64 â†’ num_classes\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Crear modelo\n",
    "if CSV_PATH.exists():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ğŸ–¥ï¸  Dispositivo: {device}\")\n",
    "    \n",
    "    model = GestureMLP(input_size=TOTAL_FEATURES, num_classes=num_classes).to(device)\n",
    "    \n",
    "    print(f\"\\nğŸ§  Modelo creado:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Contar parÃ¡metros\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ParÃ¡metros:\")\n",
    "    print(f\"   Total: {total_params:,}\")\n",
    "    print(f\"   Entrenables: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce2584",
   "metadata": {},
   "source": [
    "## ğŸ“ 8. Entrenamiento del Modelo\n",
    "\n",
    "### HiperparÃ¡metros:\n",
    "- **Epochs**: 50\n",
    "- **Batch Size**: 32\n",
    "- **Learning Rate**: 0.001\n",
    "- **Optimizer**: Adam\n",
    "- **Loss**: CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078170e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CSV_PATH.exists():\n",
    "    # HiperparÃ¡metros\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 1e-3\n",
    "    \n",
    "    # Preparar tensores\n",
    "    X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "    y_test_t = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "    \n",
    "    # DataLoader\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Loss y Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    print(\"ğŸš€ Iniciando entrenamiento...\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Listas para tracking\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    \n",
    "    # Loop de entrenamiento\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # MÃ©tricas\n",
    "            epoch_loss += loss.item() * batch_X.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_X.size(0)\n",
    "        \n",
    "        avg_loss = epoch_loss / total\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:3d}/{EPOCHS} | Loss: {avg_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nâœ… Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34bed5b",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ 9. VisualizaciÃ³n del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ae067",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CSV_PATH.exists():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(range(1, EPOCHS + 1), train_losses, linewidth=2, color='crimson')\n",
    "    axes[0].set_title('EvoluciÃ³n de la PÃ©rdida (Loss)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(range(1, EPOCHS + 1), train_accs, linewidth=2, color='forestgreen')\n",
    "    axes[1].set_title('EvoluciÃ³n de la PrecisiÃ³n (Accuracy)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ğŸ“Š Loss final: {train_losses[-1]:.4f}\")\n",
    "    print(f\"ğŸ“Š Accuracy final: {train_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f4859",
   "metadata": {},
   "source": [
    "## ğŸ¯ 10. EvaluaciÃ³n en Test Set\n",
    "\n",
    "Ahora evaluamos el modelo en datos que **nunca ha visto** durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CSV_PATH.exists():\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_t)\n",
    "        test_preds = test_outputs.argmax(dim=1).cpu().numpy()\n",
    "    \n",
    "    y_test_np = y_test_t.cpu().numpy()\n",
    "    \n",
    "    # Accuracy\n",
    "    test_acc = accuracy_score(y_test_np, test_preds)\n",
    "    \n",
    "    print(f\"ğŸ¯ Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š Classification Report:\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(\n",
    "        y_test_np, \n",
    "        test_preds, \n",
    "        target_names=le.classes_,\n",
    "        zero_division=0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805efe4d",
   "metadata": {},
   "source": [
    "## ğŸ”¥ 11. Matriz de ConfusiÃ³n\n",
    "\n",
    "Muestra quÃ© gestos se confunden entre sÃ­."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c842a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CSV_PATH.exists():\n",
    "    cm = confusion_matrix(y_test_np, test_preds)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=le.classes_,\n",
    "        yticklabels=le.classes_,\n",
    "        cbar_kws={'label': 'Cantidad'}\n",
    "    )\n",
    "    plt.title('Matriz de ConfusiÃ³n', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('PredicciÃ³n', fontsize=12)\n",
    "    plt.ylabel('Real', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ’¡ InterpretaciÃ³n:\")\n",
    "    print(\"   - Diagonal: Predicciones correctas\")\n",
    "    print(\"   - Fuera de diagonal: Confusiones entre gestos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9092b5",
   "metadata": {},
   "source": [
    "## ğŸ’¾ 12. Guardar Modelo y Artefactos\n",
    "\n",
    "Guardamos:\n",
    "1. **Modelo** (weights + arquitectura)\n",
    "2. **Scaler** (para normalizar nuevos datos)\n",
    "3. **Label Encoder** (para decodificar predicciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a1364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CSV_PATH.exists():\n",
    "    # Guardar modelo\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'input_size': TOTAL_FEATURES,\n",
    "        'num_classes': num_classes,\n",
    "        'classes': list(le.classes_),\n",
    "        'test_accuracy': test_acc,\n",
    "    }, MODEL_PATH)\n",
    "    \n",
    "    # Guardar scaler\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    \n",
    "    # Guardar encoder\n",
    "    joblib.dump(le, ENCODER_PATH)\n",
    "    \n",
    "    print(\"ğŸ’¾ Artefactos guardados:\")\n",
    "    print(f\"   âœ… Modelo: {MODEL_PATH}\")\n",
    "    print(f\"   âœ… Scaler: {SCALER_PATH}\")\n",
    "    print(f\"   âœ… LabelEncoder: {ENCODER_PATH}\")\n",
    "    print(f\"\\nğŸ‰ Â¡Todo listo para usar en producciÃ³n!\")\n",
    "    print(f\"\\nâ–¶ï¸  Ejecuta: python src/realtime_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a76240",
   "metadata": {},
   "source": [
    "## ğŸš€ 13. Resumen y PrÃ³ximos Pasos\n",
    "\n",
    "### âœ… Lo que hemos logrado:\n",
    "1. âœ… RecolecciÃ³n de datos con MediaPipe\n",
    "2. âœ… NormalizaciÃ³n de landmarks para robustez\n",
    "3. âœ… Entrenamiento de un MLP en PyTorch\n",
    "4. âœ… EvaluaciÃ³n exhaustiva del modelo\n",
    "5. âœ… Guardado de artefactos para producciÃ³n\n",
    "\n",
    "### ğŸ“ˆ Mejoras posibles:\n",
    "- ğŸ”¸ Aumentar el dataset (>500 muestras/gesto)\n",
    "- ğŸ”¸ Data augmentation (rotaciones, ruido, etc.)\n",
    "- ğŸ”¸ Probar arquitecturas mÃ¡s profundas\n",
    "- ğŸ”¸ Agregar mÃ¡s gestos\n",
    "- ğŸ”¸ Fine-tuning de hiperparÃ¡metros\n",
    "- ğŸ”¸ Usar secuencias temporales (LSTM/GRU)\n",
    "\n",
    "### ğŸ® Demo en tiempo real:\n",
    "```bash\n",
    "python src/realtime_demo.py\n",
    "```\n",
    "\n",
    "**Gestos:**\n",
    "- ğŸ‘ THUMBS_UP â†’ Subir volumen\n",
    "- ğŸ‘ THUMBS_DOWN â†’ Bajar volumen\n",
    "- â˜ï¸ INDEX_POINT â†’ Modo ratÃ³n\n",
    "- ğŸ¤ PINCH â†’ Click izquierdo\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Conceptos Clave Aprendidos\n",
    "\n",
    "1. **Computer Vision**: DetecciÃ³n de landmarks con MediaPipe\n",
    "2. **Feature Engineering**: NormalizaciÃ³n para invarianza\n",
    "3. **Deep Learning**: Arquitectura MLP con PyTorch\n",
    "4. **ML Pipeline**: Train/test split, scaling, encoding\n",
    "5. **EvaluaciÃ³n**: Accuracy, precision, recall, F1-score\n",
    "6. **ProducciÃ³n**: SerializaciÃ³n de modelos y artefactos\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ™Œ Â¡Felicidades! Has construido un sistema completo de reconocimiento de gestos.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
